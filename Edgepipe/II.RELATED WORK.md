# II. RELATED WORK
연합 학습(federated learning) 기반, 무선 손실 네트워크의 엣지 디바이스 문제를 다룸<br>
리소스와 통신 불안정성으로 인해 엣지에서 학습이 느린 문제를 해결하기 위해 파이프라인 가속을 적용함

# A. Federated Learning (연합 학습)
<mark>데이터 병렬성(data parallelism)</mark>을 사용 -> 학습 데이터를 여러 하위 작업으로 분할함<br>
데이터 병렬성: 각 장치가 전체 네트워크 모델의 복사본을 가지며 일부 학습 데이터(하위 집합 등)를 학습하는 방식<br><br>
데이터 병렬성은 신경망이 커질수록 통신 오버헤드가 증가<br>
-> 학습 장치 간의 **동기화 문제 발생**<br><br>
엣지 디바이스에서 대규모 모델의 실행 가능성 문제를 해결하기 위해 <mark>모델 병렬성(model parallelism)</mark>이 연구됨<br>
모델 병렬성: 학습 모델을 장치 간에 분할 <br>
-> 리소스 제약이 있는 엣지 디바이스에서도 대규모 학습이 가능<br>
-> 각 장치는 순차적으로 처리 …. 한계: 학습 속도가 느림<br>
이를 가속화하기 위해 Layerwise Staleness, DSP 등의 기법이 도입됨<br><br>
최근: 하이브리드 병렬성(hybrid parallelism) 접근 (모델 병렬성+데이터 병렬성)<br><br>

# B. Pipeline Acceleration
파이프라인 개념을 도입 -> 학습을 가속화하는 연구가 진행됨<br><br>
PipeDream: 파이프라인 프레임워크를 딥러닝에 적용(딥 러닝 패스로 전환)<br>
장치 간에 순차적으로 전달되는 방식(모델 병렬성처럼..?)으로 처리 성능을 최적화함<br><br>
GPipe: 미니배치를 마이크로배치로 나누어 학습 시간을 줄임 -> 더 빠르게!!<br>
HetPipe: GPU를 가상 작업자로 나눈다 … 모델과 데이터 병렬성을 결합<br>
XPipe: 데이터 배치를 더 세분화하여 처리함(미니배치는 마이크로배치로 분할)<br>
이러한 병렬화 기법은 <mark>비교적 안정적인 네트워크 연결</mark>을 전제로 함<br><br>
무선 엣지 디바이스에서의 **불안정한 연결 문제**는 지금까지 제대로 다뤄지지 않음<br>
-> 무선 네트워크의 변동성에 맞춰 파이프라인 가속을 적용하고, 슈퍼 뉴런(super neuron) 개념을 바탕으로 통신 실패 문제를 해결한 <mark>분산 학습 접근법</mark>을 제시함

