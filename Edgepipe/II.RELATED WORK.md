이 논문은 연합 학습(federated learning)을 기반으로 하며, 무선 손실 네트워크의 엣지 디바이스 문제를 다룸. 리소스와 통신 불안정성으로 인해 엣지에서 학습이 느린 문제를 해결하기 위해 파이프라인 가속을 적용함

A. Federated Learning (연합 학습)
데이터 병렬성(data parallelism)을 사용 학습 데이터를 여러 하위 작업으로 분할함
데이터 병렬성: 각 장치가 전체 네트워크 모델의 복사본을 가지며 일부 학습 데이터(하위 집합 등)를 학습하는 방식
데이터 병렬성은 신경망이 커질수록 통신 오버헤드가 증가 
-> 학습 장치 간의 동기화 문제 발생
엣지 디바이스에서 대규모 모델의 실행 가능성 문제를 해결하기 위해 모델 병렬성(model parallelism)이 연구됨
모델 병렬성: 학습 모델을 장치 간에 분할 
-> 리소스 제약이 있는 엣지 디바이스에서도 대규모 학습이 가능
-> 각 장치는 순차적으로 처리 …. 한계: 학습 속도가 느림
이를 가속화하기 위해 Layerwise Staleness, DSP 등의 기법이 도입됨
최근: 하이브리드 병렬성(hybrid parallelism) 접근 (모델 병렬성+데이터 병렬성)

B. Pipeline Acceleration
파이프라인 개념을 도입 -> 학습을 가속화하는 연구가 진행됨
PipeDream: 파이프라인 프레임워크를 딥러닝에 적용(딥 러닝 패스로 전환)
장치 간에 순차적으로 전달되는 방식(모델 병렬성처럼..?)으로 처리 성능을 최적화함
GPipe: 미니배치를 마이크로배치로 나누어 학습 시간을 줄임 -> 더 빠르게!!
HetPipe: GPU를 가상 작업자로 나눈다 … 모델과 데이터 병렬성을 결합
XPipe: 데이터 배치를 더 세분화하여 처리함(미니배치는 마이크로배치로 분할)
이러한 병렬화 기법은 비교적 안정적인 네트워크 연결을 전제로 함
무선 엣지 디바이스에서의 불안정한 연결 문제는 지금까지 제대로 다뤄지지 않음
	무선 네트워크의 변동성에 맞춰 파이프라인 가속을 적용하고, 슈퍼 뉴런(super neuron) 개념을 바탕으로 통신 실패 문제를 해결한 분산 학습 접근법을 제시함

