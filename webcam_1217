import cv2
import mediapipe as mp
import numpy as np
from fastdtw import fastdtw
from scipy.spatial.distance import euclidean
import time

# Mediapipe Pose 모델 초기화
mp_pose = mp.solutions.pose

# 전문가 영상에서 키포인트 추출 함수
def extract_keypoints_from_video(video_path):
    keypoints_list = []
    cap = cv2.VideoCapture(video_path)
    with mp_pose.Pose() as pose:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            result = pose.process(frame_rgb)
            if result.pose_landmarks:
                keypoints = np.array([[lmk.x, lmk.y, lmk.z] for lmk in result.pose_landmarks.landmark])
                keypoints_list.append(keypoints)
    cap.release()
    return np.array(keypoints_list)

# 포즈 정규화 함수 (기준점: 골반)
def normalize_keypoints(keypoints):
    pelvis = keypoints[24]  # 골반 좌표
    return keypoints - pelvis

# 프레임 리사이즈 및 잘라내기 함수
def resize_with_aspect_ratio_and_padding(frame, target_width, target_height, scale=1.5):
    frame_height, frame_width = frame.shape[:2]
    new_width = int(frame_width * scale)
    new_height = int(frame_height * scale)
    resized_frame = cv2.resize(frame, (new_width, new_height))

    x_offset = max((new_width - target_width) // 2, 0)
    y_offset = max((new_height - target_height) // 2, 0)
    cropped_frame = resized_frame[y_offset:y_offset + target_height, x_offset:x_offset + target_width]

    return cropped_frame

# 프레임 오버레이 함수
def overlay_frames(base_frame, overlay_frame, alpha=0.5):
    return cv2.addWeighted(base_frame, 1 - alpha, overlay_frame, alpha, 0)

# 각도 계산 함수
def calculate_angle(p1, p2, p3):
    a = np.array(p1)
    b = np.array(p2)
    c = np.array(p3)

    ba = a - b
    bc = c - b

    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))
    angle = np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))

    return angle

# 실시간 웹캠 입력과 전문가 포즈 비교 및 동영상 오버레이
def process_webcam_with_overlay_and_compare(expert_keypoints_list, overlay_video_path):
    normalized_expert_keypoints_list = [normalize_keypoints(kp) for kp in expert_keypoints_list]
    num_expert_frames = len(normalized_expert_keypoints_list)

    cap_webcam = cv2.VideoCapture(0)
    cap_overlay = cv2.VideoCapture(overlay_video_path)

    overlay_fps = cap_overlay.get(cv2.CAP_PROP_FPS)
    if overlay_fps == 0:
        overlay_fps = 24.0
    frame_delay = int(1000 / overlay_fps)
    frame_idx = 0
    score = 0
    feedback = "Waiting for Pose Detection..."

    # 화면 설정 (스마트폰 화면 비율)
    window_width, window_height = 607, 1080

    # 키포인트 가중치 설정
    weights = np.array([1] * 33)
    important_indices = [9, 10, 11, 12, 13, 14, 15, 16, 27, 28]  # 손목, 팔꿈치, 무릎, 발목
    weights[important_indices] = 2

    with mp_pose.Pose() as pose:
        while cap_webcam.isOpened():
            ret_webcam, frame_webcam = cap_webcam.read()
            ret_overlay, frame_overlay = cap_overlay.read()

            if not ret_webcam:
                print("Error: Webcam frame cannot be read.")
                break

            if not ret_overlay:
                cap_overlay.set(cv2.CAP_PROP_POS_FRAMES, 0)
                continue

            frame_webcam = cv2.flip(frame_webcam, 1)
            frame_webcam_resized = resize_with_aspect_ratio_and_padding(frame_webcam, window_width, window_height, scale=1.5)
            frame_overlay_resized = resize_with_aspect_ratio_and_padding(frame_overlay, window_width, window_height, scale=1.5)

            combined_frame = overlay_frames(frame_webcam_resized, frame_overlay_resized, alpha=0.5)

            # 사용자 포즈 추출
            frame_rgb = cv2.cvtColor(frame_webcam_resized, cv2.COLOR_BGR2RGB)
            result_webcam = pose.process(frame_rgb)

            if result_webcam.pose_landmarks:
                user_keypoints = np.array([[lmk.x, lmk.y, lmk.z] for lmk in result_webcam.pose_landmarks.landmark])
                normalized_user_keypoints = normalize_keypoints(user_keypoints)

                # 가중치 적용
                weighted_user_keypoints = normalized_user_keypoints * weights[:, np.newaxis]
                weighted_expert_keypoints = normalized_expert_keypoints_list[frame_idx % num_expert_frames] * weights[:, np.newaxis]

                # DTW 거리 계산
                expert_flat = weighted_expert_keypoints.flatten()
                user_flat = weighted_user_keypoints.flatten()
                distance, _ = fastdtw(expert_flat.reshape(-1, 1), user_flat.reshape(-1, 1), dist=euclidean)

                # 각도 비교 (예: 팔꿈치)
                user_elbow_angle = calculate_angle(user_keypoints[11], user_keypoints[13], user_keypoints[15])
                expert_elbow_angle = calculate_angle(normalized_expert_keypoints_list[frame_idx % num_expert_frames][11], 
                                                     normalized_expert_keypoints_list[frame_idx % num_expert_frames][13], 
                                                     normalized_expert_keypoints_list[frame_idx % num_expert_frames][15])
                angle_difference = abs(user_elbow_angle - expert_elbow_angle)

                # 점수 계산 (100점 만점 변환)
                max_distance = 40
                score = max(100 - (distance / max_distance) * 100 - angle_difference, 0)

                # 임계값 적용
                if distance > 20 or angle_difference > 30:
                    score = 0

                # 평가 점수 및 멘트
                if score >= 95:
                    feedback = "Perfect! Great job!"
                elif score >= 85:
                    feedback = "Good! You're almost there!"
                elif score >= 75:
                    feedback = "Normal! Keep going!"
                elif score >= 60:
                    feedback = "Nice try! You're getting there!"
                else:
                    feedback = "Good effort! Keep it up!"

            # 화면에 텍스트 출력
            cv2.putText(combined_frame, f"Score: {score:.2f}", (50, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(combined_frame, feedback, (50, 100),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

            frame_idx += 1

            # 화면 출력
            cv2.imshow("Webcam with Video Overlay & Pose Comparison", combined_frame)

            if cv2.waitKey(frame_delay) & 0xFF == ord('q'):
                break

    cap_webcam.release()
    cap_overlay.release()
    cv2.destroyAllWindows()

# 전문가 영상 경로 및 오버레이 동영상 경로
expert_video_path = "videos1/expert_dance1.mp4"
overlay_video_path = "videos1/bigoutput.mp4"

# 전문가 키포인트 추출
print("Extracting expert keypoints...")
expert_keypoints_list = extract_keypoints_from_video(expert_video_path)

# 함수 실행
process_webcam_with_overlay_and_compare(expert_keypoints_list, overlay_video_path)
